{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8832a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer\n",
    "from datasets import load_dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a711c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0951138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer for the RoBERTa model\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b719ead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from hugging face\n",
    "\n",
    "dataset = load_dataset(\"telord/ner-mountains-first-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d77aa71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'tokens', 'labels'],\n",
       "        num_rows: 3064\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'tokens', 'labels'],\n",
       "        num_rows: 340\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70577d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to prepare data labels for training (padding and changing label classes)\n",
    "\n",
    "def prepare_labels(data, max_length):\n",
    "    padded_data = []\n",
    "    for label in data:\n",
    "        label = list(map(lambda x: int(x != 0), label))\n",
    "        \n",
    "        if len(label) + 1 <= max_length:\n",
    "            padded_label = [0] * (max_length - (len(label) + 1)) + label + [0]\n",
    "        else:\n",
    "            padded_label = [0] + label[:max_length - 2] + [0]\n",
    "            \n",
    "        padded_data.append(padded_label)\n",
    "    return padded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4dfa038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize train data tokens\n",
    "\n",
    "train_data = tokenizer(dataset[\"train\"][\"tokens\"],\n",
    "                      truncation=True,\n",
    "                      padding=\"max_length\",\n",
    "                      max_length=MAX_LENGTH,\n",
    "                      is_split_into_words=True,\n",
    "                      return_tensors=\"pt\",\n",
    "                      return_attention_mask=True)\n",
    "\n",
    "# tokenize test data_tokens\n",
    "\n",
    "test_data = tokenizer(dataset[\"test\"][\"tokens\"],\n",
    "                      truncation=True,\n",
    "                      padding=\"max_length\",\n",
    "                      max_length=MAX_LENGTH,\n",
    "                      is_split_into_words=True,\n",
    "                      return_tensors=\"pt\",\n",
    "                      return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4403045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare train labels\n",
    "train_labels = torch.Tensor(prepare_labels(dataset[\"train\"][\"labels\"], MAX_LENGTH))\n",
    "# prepare test labels\n",
    "test_labels = torch.Tensor(prepare_labels(dataset[\"test\"][\"labels\"], MAX_LENGTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b66b8f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3064, 32]), torch.Size([340, 32]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a60c5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"labels\"] = train_labels\n",
    "test_data[\"labels\"] = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44a85723",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRoBERTaDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, labels, max_length):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ab3b4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets\n",
    "train_dataset = CustomRoBERTaDataset(train_data[\"input_ids\"],\n",
    "                                     train_data[\"attention_mask\"],\n",
    "                                     train_data[\"labels\"], max_length=MAX_LENGTH)\n",
    "\n",
    "test_dataset = CustomRoBERTaDataset(test_data[\"input_ids\"],\n",
    "                                    test_data[\"attention_mask\"],\n",
    "                                    test_data[\"labels\"], max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55061c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save datasets\n",
    "\n",
    "torch.save(train_dataset, \"data/processed_train_dataset.pt\")\n",
    "torch.save(test_dataset, \"data/processed_test_dataset.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42b971eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5003263707571801"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 0\n",
    "for lst in dataset[\"train\"][\"labels\"]:\n",
    "    lst = list(map(lambda x: int(x != 0), lst))\n",
    "    s += any(lst)\n",
    "s / len(dataset[\"train\"][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62117ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "890c66a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello!! Here you can interact with the NER model (Fine-Tuned RoBERTa)!!!!\n",
      "To make a query just enter some sentence and the model will show you what words are Mountains and what are not.\n",
      "(The mountains will be colored in blue)\n",
      "\n",
      "Below are some EXAMPLES if you cannot come up with one:\n",
      "\n",
      "    * \"So how it was on Kilimanjaro?\"\n",
      "\n",
      "    * \"White Glacier is a broad westward flowing tributary glacier which joins the Land Glacier on the north side of Mount McCoy in Marie Byrd Land .\"\n",
      "\n",
      "    * \"Other notable sections of the cemetery are the cemetery of the Finnish Guard , the Artist 's Hill and the Statesmen 's Grove .\"\n",
      "    \n",
      "    * \"Why dont we hang out together? Lets go on a trip. What about Alpas?\"\n",
      "\n",
      "Enter path to the model (recommended to use RoBERTa version):/home/user/Стільниця/python/jpnb_projects/NER_mountain/data/models/roberta_fine_tuned.pt\n",
      "Enter the sentence you want to pass into the model:So how it was on Kilimanjaro?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "HERE IS THE RESULT:\n",
      "\n",
      "\u001b[0mSo \u001b[0m how \u001b[0m it \u001b[0m was \u001b[0m on "
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AnsiFore' object has no attribute 'ORANGE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 69\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[38;5;28mprint\u001b[39m(Style\u001b[38;5;241m.\u001b[39mRESET_ALL \u001b[38;5;241m+\u001b[39m decoded_word, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)    \n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 69\u001b[0m     \u001b[43minference_ner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[60], line 63\u001b[0m, in \u001b[0;36minference_ner\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m decoded_word \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(token)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label:\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mFore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mORANGE\u001b[49m \u001b[38;5;241m+\u001b[39m decoded_word, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mprint\u001b[39m(Style\u001b[38;5;241m.\u001b[39mRESET_ALL \u001b[38;5;241m+\u001b[39m decoded_word, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AnsiFore' object has no attribute 'ORANGE'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import RobertaTokenizer\n",
    "import peft\n",
    "from colorama import Fore, Back, Style, init\n",
    "\n",
    "\n",
    "GREETING_TEXT = \"\"\"\n",
    "Hello!! Here you can interact with the NER model (Fine-Tuned RoBERTa)!!!!\n",
    "To make a query just enter some sentence and the model will show you what words are Mountains and what are not.\n",
    "(The mountains will be colored in blue)\n",
    "\n",
    "Below are some EXAMPLES if you cannot come up with one:\n",
    "\n",
    "    * \"So how it was on Kilimanjaro?\"\n",
    "\n",
    "    * \"White Glacier is a broad westward flowing tributary glacier which joins the Land Glacier on the north side of Mount McCoy in Marie Byrd Land .\"\n",
    "\n",
    "    * \"Other notable sections of the cemetery are the cemetery of the Finnish Guard , the Artist 's Hill and the Statesmen 's Grove .\"\n",
    "    \n",
    "    * \"Why dont we hang out together? Lets go on a trip. What about Alpas?\"\n",
    "\"\"\"\n",
    "MAX_LENGTH = 32\n",
    "\n",
    "def inference_ner():\n",
    "    print(GREETING_TEXT)\n",
    "    \n",
    "    path_to_model = input(\"Enter path to the model (recommended to use RoBERTa version):\")\n",
    "    sentence = input(\"Enter the sentence you want to pass into the model:\")\n",
    "    \n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    tokenized_sentence = tokenizer(sentence, \n",
    "                                   truncation=True,\n",
    "                                   padding=\"max_length\",\n",
    "                                   max_length=MAX_LENGTH,\n",
    "                                   return_tensors=\"pt\",\n",
    "                                   return_attention_mask=True,\n",
    "                                   clean_up_tokenization_spaces=True)\n",
    "    \n",
    "    model = torch.load(path_to_model)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        outputs = model(input_ids=tokenized_sentence[\"input_ids\"],\n",
    "                        attention_mask=tokenized_sentence[\"attention_mask\"])\n",
    "\n",
    "    outputs = torch.softmax(outputs.logits, dim=2).argmax(dim=2)\n",
    "    \n",
    "        \n",
    "    outputs = list(outputs.squeeze().cpu().numpy())\n",
    "    tokenized_sentence = list(tokenized_sentence[\"input_ids\"].squeeze().cpu().numpy())\n",
    "    \n",
    "    print(Fore.GREEN + \"\\nHERE IS THE RESULT:\\n\")\n",
    "    \n",
    "    for i in range(len(outputs)):\n",
    "        label = outputs[i]\n",
    "        token = tokenized_sentence[i]\n",
    "        if token in [0, 1, 2]:\n",
    "            continue\n",
    "            \n",
    "        decoded_word = tokenizer.decode(token)\n",
    "        if label:\n",
    "            print(Fore.ORANGE + decoded_word, end=\" \")\n",
    "        else:\n",
    "            print(Style.RESET_ALL + decoded_word, end=\" \")    \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inference_ner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c1fb8a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'home' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhome\u001b[49m\u001b[38;5;241m/\u001b[39muser\u001b[38;5;241m/\u001b[39mСтільниця\u001b[38;5;241m/\u001b[39mpython\u001b[38;5;241m/\u001b[39mjpnb_projects\u001b[38;5;241m/\u001b[39mNER_mountain\u001b[38;5;241m/\u001b[39mdata\u001b[38;5;241m/\u001b[39mmodels\u001b[38;5;241m/\u001b[39mroberta_fine_tuned\u001b[38;5;241m.\u001b[39mpt()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'home' is not defined"
     ]
    }
   ],
   "source": [
    "/home/user/Стільниця/python/jpnb_projects/NER_mountain/data/models/roberta_fine_tuned.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "171c64b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('This top , now known as Cadair Berwyn , is listed as Cadair Berwyn New Top on the Nuttall list .',\n",
       " [0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][\"sentence\"][24], dataset[\"train\"][\"labels\"][24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2c969601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mfsdf\n"
     ]
    }
   ],
   "source": [
    "print(Fore.LIGHTYELLOW_EX + \"fsdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e19a41c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
