{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c63ff440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import RobertaTokenizerFast\n",
    "import peft\n",
    "from colorama import Fore, Back, Style, init\n",
    "import argparse\n",
    "import warnings\n",
    "\n",
    "# Ignore any warnings to reduce console clutter.\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d527b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the maximum length for the tokenized sentence.\n",
    "MAX_LENGTH = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b8b8f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_ner(path_to_model, sentence):\n",
    "\n",
    "    # Initialize the tokenizer for RoBERTa using the pre-trained 'roberta-base' version.\n",
    "    tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', add_prefix_space=True)\n",
    "    \n",
    "    # Tokenize the user's input sentence.\n",
    "    # - Truncate the sentence if it exceeds the maximum length.\n",
    "    # - Pad the sentence to the maximum length.\n",
    "    # - Return tensors for use with PyTorch ('pt').\n",
    "    # - Include attention masks to differentiate padding from actual data.\n",
    "    tokenized_sentence = tokenizer(sentence, \n",
    "                                   truncation=True,\n",
    "                                   padding=\"max_length\",\n",
    "                                   max_length=MAX_LENGTH,\n",
    "                                   return_tensors=\"pt\",\n",
    "                                   return_attention_mask=True)\n",
    "    \n",
    "    # Load the fine-tuned model from the specified path.\n",
    "    model = torch.load(path_to_model)\n",
    "    # Set the model to evaluation mode, disabling layers like dropout.\n",
    "    model.eval()\n",
    "    \n",
    "    # Perform inference without calculating gradients (for efficiency).\n",
    "    with torch.inference_mode():\n",
    "        # Pass the tokenized input to the model, including attention masks.\n",
    "        outputs = model(input_ids=tokenized_sentence[\"input_ids\"],\n",
    "                        attention_mask=tokenized_sentence[\"attention_mask\"])\n",
    "\n",
    "    # Apply a softmax to get probabilities and find the most likely class (argmax) for each token.\n",
    "    outputs = torch.softmax(outputs.logits, dim=2).argmax(dim=2)\n",
    "\n",
    "    # Convert outputs to a list of labels and tokenized sentence to numpy arrays for processing.\n",
    "    outputs = list(outputs.squeeze().cpu().numpy())\n",
    "    tokenized_sentence = list(tokenized_sentence[\"input_ids\"].squeeze().cpu().numpy())\n",
    "    \n",
    "    # Print a message indicating that the results are about to be displayed.\n",
    "    print(Fore.GREEN + \"\\nHERE IS THE RESULT:\\n\")\n",
    "    \n",
    "    # Iterate through each token and corresponding label.\n",
    "    for i in range(len(outputs)):\n",
    "        label = outputs[i]  # The predicted label for the current token.\n",
    "        token = tokenized_sentence[i]  # The token ID from the tokenized input.\n",
    "        \n",
    "        # Skip special tokens like [CLS], [SEP], or padding.\n",
    "        if token in [0, 1, 2]:\n",
    "            continue\n",
    "            \n",
    "        # Decode the token ID back to a readable word.\n",
    "        decoded_word = tokenizer.decode(token)\n",
    "        \n",
    "        # If the label indicates a mountain entity, print the word in a different color.\n",
    "        if label:\n",
    "            print(Fore.LIGHTYELLOW_EX + decoded_word, end=\" \")\n",
    "        else:\n",
    "            # Otherwise, print the word in the default style.\n",
    "            print(Style.RESET_ALL + decoded_word, end=\" \")\n",
    "    \n",
    "    # Print a newline for better formatting after the output.\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c4cae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_examples = [\"So how it was on Kilimanjaro?\",\n",
    "                     \"White Glacier is a broad westward flowing tributary glacier which joins the Land Glacier on the north side of Mount McCoy in Marie Byrd Land.\",\n",
    "                     \"Other notable sections of the cemetery are the cemetery of the Finnish Guard, the Artist's Hill and the Statesmen's Grove.\",\n",
    "                     \"Why don't we hang out together? Let's go on a trip. What about Alpas?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a2f6b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "HERE IS THE RESULT:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 20:00:29.589525: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-24 20:00:29.667411: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-24 20:00:30.071785: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-10-24 20:00:31.408306: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m So \u001b[0m how \u001b[0m it \u001b[0m was \u001b[0m on \u001b[93m Kil \u001b[93miman \u001b[93mjar \u001b[93mo \u001b[0m? \n",
      "\u001b[32m\n",
      "HERE IS THE RESULT:\n",
      "\n",
      "\u001b[0m White \u001b[0m Glacier \u001b[0m is \u001b[0m a \u001b[0m broad \u001b[0m west \u001b[0mward \u001b[0m flowing \u001b[0m t \u001b[0mribut \u001b[0mary \u001b[0m glacier \u001b[0m which \u001b[0m joins \u001b[0m the \u001b[0m Land \u001b[0m Glacier \u001b[0m on \u001b[0m the \u001b[0m north \u001b[0m side \u001b[0m of \u001b[93m Mount \u001b[93m McCoy \u001b[0m in \u001b[0m Marie \u001b[0m Byrd \u001b[0m Land \u001b[0m. \n",
      "\u001b[32m\n",
      "HERE IS THE RESULT:\n",
      "\n",
      "\u001b[0m Other \u001b[0m notable \u001b[0m sections \u001b[0m of \u001b[0m the \u001b[0m cemetery \u001b[0m are \u001b[0m the \u001b[0m cemetery \u001b[0m of \u001b[0m the \u001b[0m Finnish \u001b[0m Guard \u001b[0m, \u001b[0m the \u001b[93m Artist \u001b[93m's \u001b[93m Hill \u001b[0m and \u001b[0m the \u001b[0m States \u001b[0mmen \u001b[0m's \u001b[0m Grove \u001b[0m. \n",
      "\u001b[32m\n",
      "HERE IS THE RESULT:\n",
      "\n",
      "\u001b[0m Why \u001b[0m don \u001b[0m't \u001b[0m we \u001b[0m hang \u001b[0m out \u001b[0m together \u001b[0m? \u001b[0m Let \u001b[0m's \u001b[0m go \u001b[0m on \u001b[0m a \u001b[0m trip \u001b[0m. \u001b[0m What \u001b[0m about \u001b[93m Al \u001b[93mpas \u001b[0m? \n"
     ]
    }
   ],
   "source": [
    "for se in sentence_examples:\n",
    "    inference_ner(\"data/models/roberta_fine_tuned.pt\", se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d960ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
